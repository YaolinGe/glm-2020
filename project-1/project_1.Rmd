---
title: "Project 1 - TMA4315"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1
**a)** Derive the log-likelihood, the score function, and expected Fisher information for a GLM where the response variable is Posson distributed for the canoncial choice of link function. 

The Poisson distribution is denoted

$$
f(y) = \frac{\lambda^y}{y!}e^{-\lambda},
$$

where $y$ is a the response of the model and $\lambda$ is the exected value of $y$. Furthermore, we apply a canonical link between the linear predictor $\eta$, holding the $M$ fixed effects $\boldsymbol{\beta}$ of the covariates $\mathbf{x}_p$ for $0<p\leq M$, and the expected value of $y$. For the Poisson likelihood the canonical link is the log-link function as  
$$
  \eta_i = \ln(\lambda_i) = \mathbf{x}_i^T \boldsymbol{\beta} \enspace\textrm{   or   } \enspace \mathbb{E}[y] = \lambda_i =\exp(\eta_i) = \exp(\mathbf{x}_i^T \boldsymbol{\beta}).
$$

Assume that the observation of the response $y_i$ are independent, then, the likelihood can be written as 
$$
 L(\beta) = \prod_{i=1}^n L_i({\beta}) = \prod_{i=1}^n f(y_i;\beta) = \prod_{i=1}^n \frac{\lambda_i^{y_i}}{y_i!}\exp(-\lambda_i),
$$
and thereby the log-likelihood is
$$
  \ell(\boldsymbol{\beta}) = \ln L(\boldsymbol{\beta}) = \sum_{i=1}^n \ell_i(\boldsymbol{\beta}) = \sum_{i=1}^n [y_i\ln(\lambda_i) -\lambda_i - \ln(y_i!)] \\
  \overset{\eta_i = \ln(\lambda_i)}{=} \sum_{i=1}^n [y_i\eta_i - \exp(\eta_i) + C_i] \overset{\eta_i=\mathbf{x}_i^T\boldsymbol{\beta}}{=} \sum_{i=1}^n \mathbf{x}_i^T\boldsymbol{\beta} - \sum_{i=1}^n \exp(\mathbf{x}_i^T\boldsymbol{\beta}) + C.
$$

The goal is now to maximize the log-likelihood by finding the optimal values of $\boldsymbol{\beta}$. This is achieved by differentiating the log-likelihood with respect to $\boldsymbol{\beta}$, set it equivalent to zero and solve it by numerically or if possible analytically. The derivative of the log-likelihood is generally refered to as the **score function** and is written as
$$
  s(\boldsymbol{\beta}) = \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = \sum_{i=1}^n \frac{\partial \ell_i(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = \sum_{i=1}^n \frac{\partial \ell_i(\boldsymbol{\beta})}{\partial \eta_i} \cdot \frac{\partial\eta_i}{\partial\boldsymbol{\beta}} = \sum_{i=1}^n \frac{\partial[y_i\eta_i - \exp(\eta_i) + C_i]}{\partial \eta_i} \cdot \frac{\partial[\mathbf{x}_i^T \boldsymbol{\beta}]}{\partial\boldsymbol{\beta}} \\
  = \sum_{i=1}^n  [y_i - \exp(\eta_i)]\cdot \mathbf{x}_i = \sum_{i=1}^n  (y_i \lambda_i)\mathbf{x}_i.
$$

The expected value of the second derivative of the log-likelihood with respect to $\boldsymbol{\beta}$ is the Fisher information, Fisher matrix or expected information matrix, and can be simplified to
$$
\mathbf{F}(\boldsymbol{\beta}) = \mathrm{Cov}[s(\boldsymbol{\beta})] = \sum_{i=1}^n \mathrm{Cov}[s_i(\boldsymbol{\beta})] = \sum_{i=1}^n \mathbb{E}\left[\left(s_i(\boldsymbol{\beta}) - \mathbb{E}[s_i(\boldsymbol{\beta})]\right)\left(s_i(\boldsymbol{\beta})-\mathbb{E}[s_i(\boldsymbol{\beta})]\right)^T\right] \\
\overset{\mathbb{E}[s_i(\boldsymbol{\beta})]=0}{=} \sum_{i=1}^n  \mathbb{E}\left[s_i(\boldsymbol{\beta})s_i(\boldsymbol{\beta})^T\right] \overset{s_i(\boldsymbol{\beta}) = (Y_i - \lambda_i)\mathbf{x}_i}{=} \sum_{i=1}^n \mathbb{E}\left[(Y_i-\lambda_i)\mathbf{x}_i(Y_i-\lambda_i) \mathbf{x}_i^T\right] \\
=\sum_{i=1}^n \mathbf{x}_i^{\hspace{0.5pt}} \mathbf{x}_i^T \mathbb{E}\left[(Y_i-\lambda_i)^2\right] \overset{\mathbb{E}\left[(Y_i-\lambda_i)^2\right]\\=\mathrm{Cov}[Y_i]=\lambda_i}{=} \sum_{i=1}^n\mathbf{x}_i^{\hspace{0.5pt}}\mathbf{x}_i^T  \lambda_i.
$$
**b)** We will now use these equations to create a $\textsf{R}$ function that iteratively calculates the maximum likelihood (ML) estimates of a particular likelihood functions. 
More spefically, the Fisher scoring algorithm for $t\geq0$ and some starting value $\boldsymbol{\beta}^{(0)}$:
$$
 \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + \mathbf{F}^{-1}(\boldsymbol{\beta}^{(t)}) \cdot S(\boldsymbol{\beta}^{(t)}),
$$
is run until the convergence criterion 
$$
\frac{||\boldsymbol{\beta}^{(t+1)}-\boldsymbol{\beta}^{(t)}||}{||\boldsymbol{\beta}^{(t)}||} \leq \epsilon,
$$
for some given $\epsilon>0$. In addition, we are interested in the deviance of the model using the ML estimates. The deviance is a discrepancy measure between observed and fitted values, and is derived as 
$$
D = 2\sum_{i=1}^n\left[\ell(\hat{\boldsymbol{\beta}})-\ell_s\right],
$$
where $\mathcal{l}_s$ is the log-likelihood of the saturated model. Moreover, for the Poisson likelihood using the log-link function the deviance can be expressed as.
$$
D = 2\sum_{i=1}^n\left[y_i \ln\left(\frac{y_i}{\hat{y}_i}\right) - (y_i - \hat{y}_i)\right],
$$
where $\hat{y}_i = \exp(\mathbf{x}_i^T\hat{\beta})$.

The function `myglm()` takes four input arguments `formula` (which covariates model the response), the `data`, the `start` ($\boldsymbol{\beta}^{(0)}$), and the $\epsilon$ convergence criterion. The ouput of `myglm()` is the ML estimates of the coefficient, the corresponding deviance with the staturated model and covariance matrix (Fisher information matrix $\mathbf{F}(\cdot)$). There are some comments in the code specifying the functionality of that particular line. In addition, we have made methods for `summary()`, `print()`, and `vcov()` for the class `myglm`.

```{r myglm}
myglm <- function(formula,data,start=NA,epsilon=4e-3){
  mf = model.frame(formula = formula, data = data) 
  # splitting up covariates and response 
  X = model.matrix(attr(mf, "terms"), data = mf) 
  y = model.response(mf)  
  
  # if start is not specified input
  if (anyNA(start)){
    beta = numeric(ncol(X))
  }
  new_beta = beta 
  crit = 100
  count = 0 
  # convergence criterion
  while (epsilon < crit){
    Sb = t(X)%*%(y-exp(X%*%beta)) # score function
    Fb = t(X)%*%as.matrix(as.data.frame(X)*exp(X%*%beta)) # fisher information
    iFb = solve(Fb) # inverse fisher information
    new_beta = beta + iFb%*%Sb # Fisher scoring iteration
    
    # calculating the convergence criterion
    crit = norm(as.matrix(new_beta - beta),type="2")/norm(as.matrix(beta),type="2")
    beta = new_beta
    count = count + 1
  }
  # storing the residuals, fitted values and the model in a list
  res = list(fitted.values = exp(X%*%new_beta),
             model = mf)
  
  # placing ML estimates in matrix 
  res$coefficients = cbind(new_beta,sqrt(diag(iFb)),new_beta/sqrt(diag(iFb)),p_value_glm(new_beta/sqrt(diag(iFb))))
  colnames(res$coefficients) = c("Estimate", "Std. Error","z value","Pr(>|z|))")
  
  i_rem = as.numeric(names(y[!is.infinite(log(y))])) # removing infinite values of log(y) 
  # calculating the deviance
  res$residuals =  log(y[i_rem]) - X[i_rem,]%*%beta
  res$deviance = 2*sum(y[i_rem]*(log(y[i_rem]) - X[i_rem,]%*%new_beta) - y[i_rem] + exp(X[i_rem,]%*%beta))
  res$null.deviance = 2*sum(y[i_rem]*(log(y[i_rem]) - log(rep(mean(y[i_rem]),length(y[i_rem])))) - y[i_rem] + rep(mean(y[i_rem]), length(y[i_rem])))
  res$aic = 2*length(new_beta) - 2*sum(y[i_rem]*(X[i_rem,]%*%new_beta) - exp(X[i_rem,]%*%beta) - log(factorial(y[i_rem])))
  
  # assigning variance 
  res$vcov = iFb
  
  # storing the function  call and formula
  res$call = match.call()
  res$formula = formula
  res$df.null = length(y)-1
  res$df.residual = length(y) - length(new_beta)
  res$iter = count
  
  # setting the class to myglm for help functions
  class(res) = "myglm"
  return(res)
}

p_value_glm <- function(z_values){
  return(2*dnorm(x = z_values))
}

# print method for myglm class object
print.myglm <- function(object){
  cat("Call:\t",format(object$call),"\n")
  cat("\nCoefficients:\n")
  print(object$coefficients[,1],digits=4)
  cat("\nDegrees of Freedom:",object$df.null,"Total (i.e. Null);", object$df.residual,"Residual\n")
  cat("Null Deviance:        ",object$null.deviance,"\n")
  cat("Residual Deviance:    ",object$deviance,"  AIC: ",object$aic)
}

# summary method for myglm class object
summary.myglm <- function(object){
  cat("Call:\n")
  cat(format(object$call),"\n")
  cat("\nDeviance Residuals:\n")
  print(digits = 4,matrix(c(min(object$residuals),as.numeric(quantile(object$residuals,0.25)),median(object$residuals),
                 as.numeric(quantile(object$residuals,0.75)),max(object$residuals)),
               dimnames = list(c("Min","1Q","Median","3Q","Max"),c()))[,1])
  cat("\nCoefficients:\n")
  print(object$coefficients)
  cat("---\n")
  cat("Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n")
  cat("(Dispersion parameter for poisson family taken to be 1)\n\n")
  cat("    Null deviance:", object$null.deviance," on ", object$df.null,"  degrees of freedom\n")
  cat("Residual deviance:", object$deviance, " on ", object$df.residual, "degrees of freedom\n")
  cat("AIC:", object$aic,"\n\n")
  cat("Number of Fisher Scoring iterations: ", object$iter)
}

# a vcov method for myglm class object
vcov.myglm <- function(object){
  print(object$vcov)
}


plot.myglm <- function(object){
  
}
```


**c)**

To test the `myglm()`function, we generate $n=100$ observations of two covariates from a uniform distribution between zero and one $x_p \sim \mathrm{U}[0,1]$ for $p=1,2$. Furthermore, we use the log-link function on a poisson likelihood function with $\boldsymbol{\beta} = (\beta_0,\beta_1,\beta_2) = (3,2,-2)$ to obtain simulations of $y$. A realization is obtained in the code below and a visualization is given in the associated figure. 

```{r generatedata}
x = matrix(c(runif(100),runif(100)),ncol = 2)
y = rpois(100,lambda = exp(3  + 2*x[,1] - 2*x[,2]))
df = data.frame(y=y,x=x)
plot(df)
```

```{r testmyglm1}
myglm1 = myglm(y~x, data = df)
myglm1 #print 
```

```{r testglm1}
glm1 = glm(y~x,family = poisson)
glm1 #print
```

```{r testmyglm2}
summary(myglm1) #summary
```

```{r testglm2}
summary(glm1) 
```

```{r testmyglm3}
vcov(myglm1) #covariance matrix
```

```{r testglm3}
vcov(glm1)
```

We observe that the output from `myglm()` function is very similar to the output from the *R* function `glm()`. 



## Exercise 2

First, we load the data: 
```{r loaddata}
load(url("https://www.math.ntnu.no/emner/TMA4315/2020h/hoge-veluwe.Rdata"))
head(data)
```
and observe that it consist of two variables $y$, fledlings leaving the nest, and $t$, the date a fledlings left the nest. We want to fit a model assuming that the number of fledlings, $y$, is Poisson distributed where the expected number of fledlings $\lambda_i$ at date $t_i$ follow a Gaussian function as 
$$
\mu_i = \lambda_i = E[Y_i]= \lambda_0 \exp\left\{ -\frac{(t_i-\theta)^2}{2\omega^2}\right\} = h(\eta_i) = \exp\{\eta_i\}.
$$

**a)**

The $\lambda_0$ parameter can be interpreted as the maximum production rate of fledlings or simply the scale of the expected value. $\theta$ sets the date of the maximum production rate of fledlings or can be viewed as a location parameter, and, lastly, $\omega$ is a shape parameter controlling the width of the Gaussian function and the slope towards maximum production rate.

**b)**
Generally the linear predictor $\eta_i$ in a generalized linear model is a addition of the effects of the covariates $\beta_p$. Using the log-link as shown in *Exercise 1*, we obtain the linear predictor:
\begin{equation}
\begin{aligned}
\eta_i & = \beta_0 + \beta_1\cdot t_i + \beta_2 \cdot t_i^2 = \ln(\mu_i) \\
& = \ln(\lambda_i) = \ln(\lambda_0) - \frac{(t_i-\theta)^2}{2\omega^2} \\
&= \ln(\lambda_0) - \frac{1}{2\omega^2}\cdot t_i^2 + \frac{\theta}{\omega^2}\cdot t_i - \frac{\theta^2}{2\omega^2}.
\end{aligned}
\end{equation}
Here, we see that the linear predictor using the specified expectation $\lambda$ can be expressed as a additive linear predictor that is linked to the mean of the response through the log-link function. The model is therefore a GLM with a Poisson likelihood. 

In the above reparametrization, we observe the relationship coefficients $(\beta_0,\beta_1,\beta_2)$ and $(\lambda_0,\theta,\omega)$ is
$$
\beta_0  = \ln(\lambda_0) - \frac{\theta^2}{2\omega^2} \enspace ;\enspace \beta_1 = \frac{\theta}{\omega^2} \enspace ;\enspace\beta_2  = -\frac{1}{2\omega^2}.
$$
Assuming that $(\beta_0,\beta_1,\beta_2)$ are known, we can find the unknown parameters by reformulating the above expression as
$$
\omega = \frac{1}{\sqrt{-2\beta_2}}\enspace ; \enspace
\theta = -\frac{\beta_1}{2\beta_2} \enspace ; \enspace
\lambda_0 = \exp\left\{\beta_0 - \frac{\beta_1^2}{2\beta_2}\right\}.
$$

**c)**


```{r fitdata}
res = myglm(y~t+I(t^2),data=data)
res
```


**d)**
```{r summaryquad}
summary(res)
```
From the *p*-value in the summary output above we observe taht the quadratic effect is signicant, i.e. there is evidence of a quadrativ effect of $t$. 

**e)**
```{r testy}
library(ggplot2)
ggplot(data = data.frame(data), aes(x=t,y=y)) + 
  geom_point()
```

**f)**

$Z = f(x,y)$ 

```{r transformfit}
omega = 1/sqrt(-2*res$coefficients[3,1])
theta = - res$coefficients[2,1] /(2*res$coefficients[3,1])
lambda0 = exp(res$coefficients[1,1] - res$coefficients[2,1]^2/(2*res$coefficients[3,1]))
cat("lambda0 =", lambda0, "\ntheta =",theta, "\nomega =", omega)
```


$$
 \mathrm{Var}(Z) = \left(\frac{\partial f} {\partial x}\right)^2  \mathrm{Var}(X) + \left(\frac{\partial f} {\partial y}\right)^2  \mathrm{Var}(Y)  + 2 \frac{\partial f} {\partial x}\frac{\partial f} {\partial y}  \mathrm{Cov}(X,Y)
$$

$$
 \mathrm{Var}(\theta) = \left(\frac{\partial \theta} {\partial \beta_1}\right)^2  \mathrm{Var}(\beta_1) + \left(\frac{\partial \theta} {\partial \beta_2}\right)^2  \mathrm{Var}(\beta_2)  + 2 \frac{\partial \theta} {\partial \beta_1}\frac{\partial \theta} {\partial \beta_2}  \mathrm{Cov}(\beta_1,\beta_2) \\
 = \left(-\frac{1} {2\beta_2}\right)^2  \mathrm{Var}(\beta_1) + \left(\frac{\beta_1}{2\beta_2^2}\right)^2  \mathrm{Var}(\beta_2)  + 2 \frac{-1}{2 \beta_2}\frac{\beta_1} {2\beta_2^2}  \mathrm{Cov}(\beta_1,\beta_2) \\
 = \frac{1} {4\beta_2^2} \mathrm{Var}(\beta_1) + \frac{\beta_1^2}{4\beta_2^4} \mathrm{Var}(\beta_2) - \frac{\beta_1}{2 \beta_2^3}\mathrm{Cov}(\beta_1,\beta_2) 
$$

```{r sdtheta}
sd_theta = sqrt(1/(4*res$coefficients[3,1]^2)*res$vcov[2,2] + res$coefficients[2,1]^2/(4*res$coefficients[3,1]^4)*res$vcov[3,3] - res$coefficients[2,1]/(2*res$coefficients[3,1]^3)*res$vcov[2,3])
sd_theta
```

$$
 \mathrm{Var}(\omega) = \left(\frac{\partial \omega} {\partial \beta_2}\right)^2  \mathrm{Var}(\beta_2) 
 = \left(\frac{1}{2\sqrt{2}(-\beta_2)^{\frac{3}{2}}}\right)^2  \mathrm{Var}(\beta_2)  
 = \frac{1}{8\cdot(-\beta_2)^3}\mathrm{Var}(\beta_2)
$$

```{r sdomega}
sd_omega = sqrt(1/(8*(-res$coefficients[3,1])^(3))*res$vcov[3,3])
sd_omega
```

$$
 \mathrm{Var}(\lambda_0) =\left(\frac{\partial \lambda_0} {\partial \beta_0}\right)^2  \mathrm{Var}(\beta_0) + \left(\frac{\partial \lambda_0} {\partial \beta_1}\right)^2  \mathrm{Var}(\beta_1) +  \left(\frac{\partial \lambda_0} {\partial \beta_2}\right)^2  \mathrm{Var}(\beta_2)  +  2\frac{\partial \lambda_0} {\partial \beta_0}\frac{\partial \lambda_0} {\partial \beta_1}  \mathrm{Cov}(\beta_0,\beta_1) +  2\frac{\partial \lambda_0} {\partial \beta_0}\frac{\partial \lambda_0} {\partial \beta_2}  \mathrm{Cov}(\beta_0,\beta_2)  + 2\frac{\partial \lambda_0} {\partial \beta_1}\frac{\partial \lambda_0} {\partial \beta_2}  \mathrm{Cov}(\beta_1,\beta_2) \\
 = \exp\left[ 2\left(\beta_0 - \frac{\beta_1^2}{2\beta_2}\right)\right]\cdot\left[\mathrm{Var}(\beta_0) + \frac{\beta_1^2}{\beta_2^2}\mathrm{Var}(\beta_1) + \frac{1}{4}\frac{\beta_1^4}{\beta_2^4}\mathrm{Var}(\beta_2) - 2 \frac{\beta_1}{\beta_2}\mathrm{Cov}(\beta_0,\beta_1) + \frac{\beta_1^2}{\beta_2^2}\mathrm{Cov}(\beta_0,\beta_2) -
 \frac{\beta_1^3}{\beta_2^3} \mathrm{Cov}(\beta_1,\beta_2) 
 \right]
$$

```{r sdlambda0}
sd_lambda_0 = sqrt(
  exp(
    2*(res$coefficients[1,1] - res$coefficients[2,1]^2/(2*res$coefficients[3,1])))*
    (res$vcov[1,1] + 
       res$coefficients[2,1]^2/res$coefficients[3,1]^2*res$vcov[2,2] + 
       res$coefficients[2,1]^4/res$coefficients[3,1]^4*res$vcov[3,3]/4 - 
       2*res$coefficients[2,1]/res$coefficients[3,1]*res$vcov[1,2] + 
       res$coefficients[2,1]^2/res$coefficients[3,1]^2*res$vcov[1,3] - 
       res$coefficients[2,1]^3/res$coefficients[3,1]^3*res$vcov[2,3])
  )
sd_lambda_0
```


**g)**



### Exercise 3


```{r paraboot}
pboot <- function(formula,data,start=NA,epsilon=4e-3,trials=1000){
  mf = model.frame(formula = formula, data = data) 
  # splitting up covariates and response 
  X = model.matrix(attr(mf, "terms"), data = mf) 
  y = model.response(mf)
  # if start is not specified input
  if (anyNA(start)){
    start = numeric(ncol(X))
  }
  betas = matrix(NA,nrow = trials, ncol = ncol(X))
  for (i in seq(trials)){
    tmp_data = data[sample(seq(nrow(data)),nrow(data),replace=T),]
    new_beta = beta = start
    crit = 100 
    mf = model.frame(formula = formula, data = tmp_data) 
    # splitting up covariates and response 
    X = model.matrix(attr(mf, "terms"), data = mf) 
    y = model.response(mf)
    # convergence criterion
    while (epsilon < crit){
      Sb = t(X)%*%(y-exp(X%*%beta)) # score function
      Fb = t(X)%*%as.matrix(as.data.frame(X)*exp(X%*%beta)) # fisher information
      iFb = solve(Fb) # inverse fisher information
      new_beta = beta + iFb%*%Sb # Fisher scoring iteration
    
      # calculating the convergence criterion
      crit = norm(as.matrix(new_beta - beta),type="2")/norm(as.matrix(beta),type="2")
      beta = new_beta
    }
    betas[i,] = new_beta
  }
  res = list(coefficients=cbind(colMeans(betas),apply(betas, 2, sd)))
  res$glm = myglm(formula,data)
  res$call = match.call()
  rownames(res$coefficients) = rownames(res$glm$coefficients[,c(1,2)])
  colnames(res$coefficients) = colnames(res$glm$coefficients[,c(1,2)])
  class(res) = "pboot"
  return(res)
}
print.pboot <- function(object){
  cat("Call:\t",format(object$call),"\n")
  cat("\nParametric bootstrapping:\n")
  print(object$coefficients,digits=4)
  cat("\nFisher information:\n")
  print(object$glm$coefficients[,c(1,2)],digits=4)
}

```


```{r testboot,eval = F}
res_boot = pboot(y~t+I(t^2),data=data)
res_boot
```

